{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f4924f2-018e-4e36-9061-768e020fd0e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1693307783429,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "9f4924f2-018e-4e36-9061-768e020fd0e7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imresize\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "#from google.colab import drive\n",
    "import warnings\n",
    "#ignore('warnings')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ckgTNXAGu_cN",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693307783430,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "ckgTNXAGu_cN"
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e59b726-f656-40ee-a94a-d6960b3e15a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3183,
     "status": "ok",
     "timestamp": 1693307786607,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "0e59b726-f656-40ee-a94a-d6960b3e15a0",
    "outputId": "9804f681-93c9-45a1-9e0a-c4895e8d89fa"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "mP4c5jpm90Nd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79637,
     "status": "ok",
     "timestamp": 1693307866238,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "mP4c5jpm90Nd",
    "outputId": "9724ad83-101d-400d-de70-1f2d0839b638"
   },
   "outputs": [],
   "source": [
    "#!unzip /content/gdrive/My\\ Drive/DeepLearning/GestureRecog_CaseStudy/Project_data.zip\n",
    "#!ls\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ODw25Uzbv0Zd",
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1693310099812,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "ODw25Uzbv0Zd"
   },
   "outputs": [],
   "source": [
    "\n",
    "#train_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/train.csv').readlines())\n",
    "#val_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/val.csv').readlines())\n",
    "#batch_size = #experiment with the batch size\n",
    "\n",
    "train_doc = np.random.permutation(open('C:\\\\Users\\\\rajansa\\\\OneDrive - Lam Research\\\\QA Folder\\\\_myLearning\\\\DeepLearning\\\\Gesture_Recognition\\\\Project_data\\\\train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('C:\\\\Users\\\\rajansa\\\\OneDrive - Lam Research\\\\QA Folder\\\\_myLearning\\\\DeepLearning\\\\Gesture_Recognition\\\\Project_data\\\\val.csv').readlines())\n",
    "batch_size = 64 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6648297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fz7KGShRWKuX",
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1693314345302,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "fz7KGShRWKuX"
   },
   "outputs": [],
   "source": [
    "def generator1(source_path, folder_list, batch_size):\n",
    "  #print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "  print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "  img_idx = 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17  #create a list of image numbers you want to use for a particular video\n",
    "  #img_idx1 = 0,1,2\n",
    "  x = len(img_idx)\n",
    "  print(\"Length of Source Path\",len(source_path))\n",
    "  print(\"Length of batch Size\",batch_size)\n",
    "  while True:\n",
    "    t = np.random.permutation(folder_list)\n",
    "    num_batches = len(source_path)//batch_size # calculate the number of batches\n",
    "    for batch in range(num_batches): # we iterate over the number of batches\n",
    "      batch_data = np.zeros((batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "      batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      for folder in range(batch_size): # iterate over the batch_size\n",
    "        imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "          image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "          #crop the images and resize them. Note that the images are of 2 different shape\n",
    "          #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "          \n",
    "          #h, w, c = image.shape\n",
    "          #print(image.shape)\n",
    "          #image = resize(image, (height, width), anti_aliasing=True)\n",
    "          if image.shape[1] == 160:\n",
    "            image = resize(image[:,20:140,:],(100,100)).astype(np.float32)\n",
    "          else:\n",
    "            #print(\"\")\n",
    "            image=resize(image,(100,100)).astype(np.float32)\n",
    "\n",
    "          batch_data[folder,idx,:,:,0] = image[:,:,0] - 104 #normalise and feed in the image\n",
    "          batch_data[folder,idx,:,:,1] = image[:,:,1] - 117  #normalise and feed in the image\n",
    "          batch_data[folder,idx,:,:,2] = image[:,:,2] - 123  #normalise and feed in the image\n",
    "\n",
    "          batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "      yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "\n",
    "\n",
    "    # write the code for the remaining data points which are left after full batches\n",
    "    #print(len(t))\n",
    "    if(len(t)%batch_size!=0):\n",
    "      #new_batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "      #print(\"New Batch Size\", new_batch_size)\n",
    "      batch_data = np.zeros((len(t)%batch_size,18,100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "      #print(\"New Batch Size\", len(t)%batch_size)\n",
    "      batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      #print(\"New Batch Size\", new_batch_size)\n",
    "      #print(\"New Batch Size\", type(new_batch_size))\n",
    "      for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
    "        #print(\"folder Batch Size\", folder)\n",
    "        #print(\"folder Batch Size\", type(folder))\n",
    "        imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*len(t)%batch_size)].split(';')[0]) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "          image = imageio.imread(source_path+'/'+ t[folder + (num_batches*len(t)%batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "          #crop the images and resize them. Note that the images are of 2 different shape\n",
    "          #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "          if image.shape[1] == 160:\n",
    "            image = resize(image[:,20:140,:],(100,100)).astype(np.float32)\n",
    "          else:\n",
    "            image = resize(image,(100,100)).astype(np.float32)\n",
    "          # Normalizing RGB image with mean subtraction\n",
    "          batch_data[folder,idx,:,:,0] = image[:,:,0] - 104   #normalise and feed in the image\n",
    "          batch_data[folder,idx,:,:,1] = image[:,:,1] - 117   #normalise and feed in the image\n",
    "          batch_data[folder,idx,:,:,2] = image[:,:,2] - 123   #normalise and feed in the image\n",
    "\n",
    "          #batch_data[folder,idx,:,:,0] = batch_data[folder,idx,:,:,0]/255 #normalise and feed in the image\n",
    "          #batch_data[folder,idx,:,:,1] = batch_data[folder,idx,:,:,1]/255  #normalise and feed in the image\n",
    "          #batch_data[folder,idx,:,:,2] = batch_data[folder,idx,:,:,2]/255  #normalise and feed in the image\n",
    "\n",
    "          batch_labels[folder, int(t[folder + (num_batches*len(t)%batch_size)].strip().split(';')[2])] = 1\n",
    "      yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5JslQuBNUsx2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1693307866240,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "5JslQuBNUsx2",
    "outputId": "18e5d024-302f-4aee-e227-208970a8f019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'C:\\\\Users\\\\rajansa\\\\OneDrive - Lam Research\\\\QA Folder\\\\_myLearning\\\\DeepLearning\\\\Gesture_Recognition\\\\Project_data\\\\train'\n",
    "val_path = 'C:\\\\Users\\\\rajansa\\\\OneDrive - Lam Research\\\\QA Folder\\\\_myLearning\\\\DeepLearning\\\\Gesture_Recognition\\\\Project_data\\\\val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "K03D9fN2UeVY",
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1693313347411,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "K03D9fN2UeVY"
   },
   "outputs": [],
   "source": [
    "train_generator = generator1(train_path, train_doc, batch_size)\n",
    "#print(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tynppjuyV-6I",
   "metadata": {
    "id": "tynppjuyV-6I"
   },
   "source": [
    "## Model\n",
    "\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "rnnY9ZUCV9vT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 563,
     "status": "error",
     "timestamp": 1693314999272,
     "user": {
      "displayName": "santhosh kumar",
      "userId": "17400671179659829837"
     },
     "user_tz": -330
    },
    "id": "rnnY9ZUCV9vT",
    "outputId": "dc70885f-3a4d-4a97-fe3f-fb7c3630b925"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "model = Sequential()\n",
    "model.add(Conv3D(64,(3,3,3),padding='same',input_shape=(18,100, 100,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model.add(Conv3D(128,(3,3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "#model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,1)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(256,(3,3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(256,(3,3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation = 'elu'))\n",
    "#model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(len(class_names)))\n",
    "model.add(Dense(5,activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "232e24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 18, 100, 100, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 18, 100, 100, 64)  256      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 18, 100, 100, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPoolin  (None, 9, 50, 100, 64)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 9, 50, 100, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 9, 50, 100, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 9, 50, 100, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPoolin  (None, 4, 25, 50, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 4, 25, 50, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 4, 25, 50, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 4, 25, 50, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPoolin  (None, 2, 12, 25, 256)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 2, 12, 25, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 2, 12, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 2, 12, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPoolin  (None, 1, 6, 12, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               9437696   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,324,357\n",
      "Trainable params: 12,322,949\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.SGD(learning_rate=0.001,momentum=0.9,nesterov=True) \n",
    " #,weight_decay=None,clipnorm=None,clipvalue=None,global_clipnorm=None,use_ema=False,ema_momentum=0.99,ema_overwrite_frequency=None,jit_compile=True,) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "48a17c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_generator = generator1(train_path, train_doc, batch_size)\n",
    "val_generator = generator1(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3ed4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience =2, verbose=1, mode='min',min_delta=0.0001, cooldown=0, min_lr = 0.00001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d68de9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4a2bbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a7a587c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\Users\\rajansa\\OneDrive - Lam Research\\QA Folder\\_myLearning\\DeepLearning\\Gesture_Recognition\\Project_data\\train ; batch size = 64\n",
      "Length of Source Path 114\n",
      "Length of batch Size 64\n",
      "Epoch 1/30\n",
      "11/11 [==============================] - ETA: 0s - loss: 4.8779 - categorical_accuracy: 0.3327 Source path =  C:\\Users\\rajansa\\OneDrive - Lam Research\\QA Folder\\_myLearning\\DeepLearning\\Gesture_Recognition\\Project_data\\val ; batch size = 64\n",
      "Length of Source Path 112\n",
      "Length of batch Size 64\n",
      "\n",
      "Epoch 1: saving model to model_init_2023-08-3115_02_13.740031\\model-00001-4.87794-0.33267-17.33815-0.19000.h5\n",
      "11/11 [==============================] - 809s 72s/step - loss: 4.8779 - categorical_accuracy: 0.3327 - val_loss: 17.3381 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/30\n",
      " 5/11 [============>.................] - ETA: 5:56 - loss: 2.6107 - categorical_accuracy: 0.4112"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1752\\2316749052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "#                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,validation_steps=validation_steps, \n",
    "                    class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7b3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
